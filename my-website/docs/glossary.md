---
sidebar_position: 10 # Adjust as needed
title: Glossary
---

# Glossary of Terms

This glossary provides definitions for key terms and concepts used throughout the book "Humanoid Robotics & Embodied Intelligence".

## A

## C

**Control Commands**: Instructions sent to a robot to dictate its actions and movements. These can vary from low-level joint commands to high-level action requests.

**Course Module**: A defined unit of learning within the book, structuring content for incremental progression and specific learning outcomes.

## D

**Digital Twin**: A virtual representation of a physical object or system, used for simulation, testing, and analysis in a digital environment.

**Dynamics**: The study of forces and torques and their effect on the motion of objects, crucial for understanding how robots move and interact with their environment.

## E

**Embodied Intelligence**: The concept that an intelligent agent (like a robot) requires a physical body and real-world interaction to develop and exhibit true intelligence.

**Environment Map**: A representation of a robot's surroundings, typically built using sensor data, which allows the robot to localize itself and navigate.

## G

**Gazebo**: An open-source 3D robotics simulator widely used for testing robotics algorithms, designing robots, and performing regression testing.

## H

**Humanoid Robot Model**: A digital or physical representation of a robot designed to resemble and/or mimic human form and movement.

## I

**IMU (Inertial Measurement Unit)**: An electronic device that measures and reports a body's specific force, angular rate, and sometimes the orientation of the body, using a combination of accelerometers, gyroscopes, and magnetometers.

**Isaac Sim**: NVIDIA's scalable robotics simulation application and synthetic data generation tool, built on the Omniverse platform, for developing and testing AI-enabled robots.

## K

**Kinematics**: The study of motion without considering the forces that cause that motion, describing the geometry of motion for robot linkages and joints.

## L

**LiDAR (Light Detection and Ranging)**: A remote sensing method that uses pulsed laser to measure ranges (variable distances) to the Earth. In robotics, it's used for mapping and obstacle detection.

## N

**Natural Language Commands**: Text-based instructions given to a robot system, often processed by Vision-Language-Action (VLA) systems, to perform complex tasks.

## O

**Occupancy Grid**: A type of environment map used in robotics where the environment is discretized into a grid, and each cell stores the probability of being occupied by an obstacle.

## R

**ROS 2 (Robot Operating System 2)**: An open-source, meta-operating system for robots, providing services like hardware abstraction, low-level device control, inter-process message-passing, and package management.

## S

**SDF (Simulation Description Format)**: An XML format for describing objects and environments for robot simulators like Gazebo.

**Sensor Data**: Information collected by a robot's sensors (e.g., cameras, LiDAR, IMUs) about its internal state and external environment.

**SLAM (Simultaneous Localization and Mapping)**: A computational problem of constructing or updating a map of an unknown environment while simultaneously keeping track of an agent's location within it.

## U

**Unity**: A cross-platform game engine that is also used as a powerful platform for robotics simulation, especially for visually rich and interactive environments.

**URDF (Unified Robot Description Format)**: An XML file format used in ROS to describe all elements of a robot, including its visual appearance, collision properties, and inertial properties.

## V

**Vision-Language-Action (VLA) Systems**: AI systems that integrate visual perception, natural language understanding, and physical action capabilities to enable robots to respond to human commands in complex environments.